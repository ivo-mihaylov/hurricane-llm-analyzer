# ğŸ§  Local LLM Question Answering App
![CI](https://github.com/ivo-mihaylov/hurricane-llm-analyzer/actions/workflows/ci.yml/badge.svg)

# Hurricane LLM Analyzer

This project is a simple, containerized, local LLM-powered web application that allows you to query a hurricane dataset in natural language. It uses:

- A **Flask** web interface
- The **Mistral** large language model
- Hosted locally via **Ollama**
- Dockerized for easy testing and sharing

---

## ğŸ”— Final URL for Testing
Access the app at:
```
http://127.0.0.1:5001
```
> **Note:** Even though Flask internally runs on port `5000`, it is mapped to host port `5001` in Docker. Trying `http://127.0.0.1:5000` will result in **access denied**.

---

## ğŸš€ How to Run the App

This project is fully containerized using Docker, meaning **you do NOT need to install Python, Ollama, or any dependencies manually**.

### âœ… Prerequisites
- [Docker Desktop](https://www.docker.com/products/docker-desktop/) installed and running on your machine.
- At least **8â€“10 GB of free disk space** for model pulling.
- Internet connection (required to download the base Docker images and the Mistral model on first run).

### ğŸ§­ Steps to Run

1. **Clone the project (if not already):**

   ```bash
   git clone https://github.com/your-username/your-repo-name.git
   cd your-repo-name
   ```

2. **Run the containers using Docker Compose:**

   ```bash
   docker compose up --build
   ```

   > This will:
   > - Start the **Flask** application
   > - Spin up **Ollama** in a separate container
   > - Automatically download and launch the **Mistral** language model
   > - Wait for Ollama to be ready before starting the app

3. **Open your browser and go to:**

   ğŸ‘‰ [http://127.0.0.1:5001](http://127.0.0.1:5001)

   > âš ï¸ **Note:** Do **NOT** use `http://127.0.0.1:5000` even if the logs suggest it â€” the app is exposed on **port 5001**, as defined in `docker-compose.yml`.

---

## ğŸ“ Project Structure & Purpose

```bash
LLM/
â”œâ”€â”€ app.py                      # Legacy entry point (not used in container setup)
â”œâ”€â”€ docker-compose.yml         # Spins up Flask and Ollama containers
â”œâ”€â”€ Dockerfile                 # For Flask app container
â”œâ”€â”€ Dockerfile.ollama          # Custom Ollama container with Mistral pre-installed
â”œâ”€â”€ entrypoint.sh              # Waits for Ollama before starting Flask
â”œâ”€â”€ wait-for-ollama.sh         # Ensures Ollama is healthy before Flask launches
â”œâ”€â”€ requirements.txt           # Flask + dependencies
â”œâ”€â”€ README.md                  # Project documentation
â”œâ”€â”€ web_app.py                 # Main Flask application
â”œâ”€â”€ .env                       # Environment variables (optional)
â”œâ”€â”€ main/
â”‚   â”œâ”€â”€ data.py                # Hurricane dataset
â”‚   â”œâ”€â”€ data_loader.py         # Loads dataset
â”‚   â””â”€â”€ llm_interface.py       # Handles query-to-LLM communication
â””â”€â”€ tests/
    â””â”€â”€ test_web_app.py        # Sample test case
```

---

## âš™ï¸ How It Works â€“ Execution Flow

1. **User** opens `http://127.0.0.1:5001` in browser
2. **Flask** (`web_app.py`) captures natural language query
3. It forwards the query to **Ollama's Mistral API** (running at `http://ollama:11434`)
4. **Mistral** returns an answer, which is displayed back in the browser

---

## ğŸ§© How the Pieces Work Together

### ğŸ“„ `web_app.py`
- Renders UI and handles HTTP requests
- Sends queries to `llm_interface.py`

### âš™ï¸ `llm_interface.py`
- Sends the user prompt to Mistral via Ollama API
- Receives and returns the model's response

### ğŸ“Š `data_loader.py`
- Loads the hurricane CSV file into pandas for future enhancement

### ğŸ“¦ `Dockerfile`
- Builds the Flask container using Python 3.12

### ğŸ§  `Dockerfile.ollama`
- Builds a custom Ollama image that already includes Mistral model

### ğŸ•’ `entrypoint.sh` / `wait-for-ollama.sh`
- Ensures Flask only starts once Ollama is ready and healthy

---

## ğŸ› ï¸ Changes Made (Compared to Initial Version)

### âœ… Containerized Ollama
- **Why:** Originally, testers needed to install Ollama locally. Docker now handles this.

### âœ… Switched to Docker Compose with 2 Services
- **Why:** Needed Flask and Ollama to run together in sync.

### âœ… Added Health Checks & Entrypoint Wait Script
- **Why:** Flask was crashing when Ollama wasn't ready. The wait script solves this.

### âœ… Flask port changed to 5001 on host
- **Why:** Port 5000 was often in use and caused collisions. 5001 avoids this.

### âœ… Pre-installed Mistral inside Dockerfile.ollama
- **Why:** So users donâ€™t wait 15+ mins on first run, or need internet.

### âœ… CI/CD setup (in `.github/workflows/`)
- **Why:** Ensures automated testing and clean GitHub pushes

---

## ğŸ§ª Issue Faced: Missing Ollama Locally

### âŒ Initial Error
```bash
ConnectionError: Failed to establish new connection: [Errno 111] Connection refused
```
- **Cause:** Userâ€™s machine didnâ€™t have Ollama installed
- **Fix:** Added a dedicated Ollama container via Docker with pre-pulled Mistral model

---

## ğŸ” Final Docker Flow

1. `docker-compose up --build`
2. Spins up:
    - `ollama` container (w/ Mistral pre-installed)
    - `flask_app` container
3. You can now visit `http://127.0.0.1:5001` and chat with the model!

---

## ğŸ“Š Visual Diagram
(See attached image below for visual representation of file interactions and flow)

> Separate PNG file: `llm_project_architecture.png`



## ğŸ‘©â€ğŸ’» Created by
Ivo Mihaylov - a A Junior Software Automation Engineer applicant
